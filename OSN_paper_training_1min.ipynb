{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8f63cef",
   "metadata": {},
   "source": [
    "# Flow Matching Model Training\n",
    "\n",
    "This notebook trains the Flow Matching Model for generative flight trajectory prediction. The model learns to predict future aircraft trajectories given historical flight data and contextual information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3145f888",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37292f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from traffic.core import Traffic\n",
    "\n",
    "# Quiet optional logs for cleaner output\n",
    "os.environ[\"TRITON_PRINT_AUTOTUNING\"] = \"0\"\n",
    "if \"TORCH_LOGS\" in os.environ:\n",
    "    os.environ.pop(\"TORCH_LOGS\", None)\n",
    "\n",
    "# Import project utilities\n",
    "from utils import (\n",
    "    load_and_engineer,\n",
    "    WindowParams,\n",
    "    SplitConfig,\n",
    "    SamplingConfig,\n",
    "    StatsConfig,\n",
    "    CFMDataset,\n",
    "    make_loader,\n",
    "    build_or_load_dataset,\n",
    "    TurnSampling\n",
    ")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Enable optimizations if using CUDA\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe86dfe",
   "metadata": {},
   "source": [
    "## Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51b95bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data path\n",
    "INPUT_PARQUET = \"trajs_LSAS_filtered.parquet\"\n",
    "\n",
    "# Load and perform feature engineering\n",
    "print(\"Loading and engineering trajectory data...\")\n",
    "df = load_and_engineer(INPUT_PARQUET)\n",
    "print(f\"Loaded dataset with {len(df)} trajectory segments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da998122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "wparams = WindowParams(\n",
    "    input_len=60,      # 60 historical time steps (5 minutes at 5Hz)\n",
    "    output_horizon=60,  # 60 future time steps (5 minutes at 5Hz)\n",
    "    output_stride=5,    # 5 second intervals\n",
    "    overlap=False       # No overlap between windows\n",
    ")\n",
    "\n",
    "# Train/validation/test split configuration\n",
    "scfg = SplitConfig(\n",
    "    train_frac=0.8,   # 80% training\n",
    "    val_frac=0.1,     # 10% validation\n",
    "    split_seed=42     # Reproducible splits\n",
    ")\n",
    "\n",
    "# Sampling configuration for trajectory selection\n",
    "samp = SamplingConfig(\n",
    "    n_train=1_000_000,  # 1M training samples\n",
    "    n_val=200_000,      # 200K validation samples\n",
    "    n_test=200_000,     # 200K test samples\n",
    "    train_turn=TurnSampling(\n",
    "        min_turn_frac=0.30, turn_thr=0.01, consec=3,\n",
    "        consider_hist=True, consider_future=True\n",
    "    ),\n",
    "    val_turn=TurnSampling(\n",
    "        min_turn_frac=0.30, turn_thr=0.01, consec=3,\n",
    "        consider_hist=True, consider_future=True\n",
    "    ),\n",
    "    test_turn=TurnSampling(\n",
    "        min_turn_frac=0.0, turn_thr=0.01, consec=3,\n",
    "        consider_hist=True, consider_future=True\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Normalization statistics configuration\n",
    "stats_cfg = StatsConfig(\n",
    "    stats_seed=1234,\n",
    "    stats_sample_size=2_000_000  # Sample size for computing normalization stats\n",
    ")\n",
    "\n",
    "# Build or load processed datasets\n",
    "print(\"Building datasets...\")\n",
    "(X_train, Y_train, C_train,\n",
    " X_val, Y_val, C_val,\n",
    " X_test, Y_test, C_test,\n",
    " norm_stats, meta_train, meta_val, meta_test,\n",
    " manifest, summary) = build_or_load_dataset(df, wparams, scfg, samp, stats_cfg)\n",
    "\n",
    "# Create dataset objects\n",
    "train_ds = CFMDataset(X_train, Y_train, C_train)\n",
    "val_ds = CFMDataset(X_val, Y_val, C_val)\n",
    "test_ds = CFMDataset(X_test, Y_test, C_test)\n",
    "\n",
    "print(f\"Dataset sizes: train={len(train_ds)}, val={len(val_ds)}, test={len(test_ds)}\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = make_loader(train_ds, bs=2048, shuffle=True)\n",
    "val_loader = make_loader(val_ds, bs=2048, shuffle=False)\n",
    "test_loader = make_loader(test_ds, bs=2048, shuffle=False)\n",
    "\n",
    "print(f\"Batch counts: train={len(train_loader)}, val={len(val_loader)}, test={len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c8c2af",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "Import the Flow Matching Model components for conditional generative trajectory prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221d4c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model architecture and utilities from dedicated module\n",
    "from model import FlowMatchingModel, sample_xt_and_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aa71e8",
   "metadata": {},
   "source": [
    "## Training Utilities\n",
    "\n",
    "Import training utilities including learning rate scheduling and EMA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a4b108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import training utilities from dedicated module\n",
    "from training_utils import train_cfm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dc42a2",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Execute the training process with the configured hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feaf61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract normalization statistics\n",
    "feat_mean = norm_stats[\"feat_mean\"]\n",
    "feat_std = norm_stats[\"feat_std\"]\n",
    "ctx_mean = norm_stats[\"ctx_mean\"]\n",
    "ctx_std = norm_stats[\"ctx_std\"]\n",
    "\n",
    "# Model configuration\n",
    "model_cfg = dict(\n",
    "    d_model=512,        # Model dimension\n",
    "    nhead=8,           # Number of attention heads\n",
    "    enc_layers=6,      # Encoder layers\n",
    "    dec_layers=8,      # Decoder layers\n",
    "    ff=4*512,          # Feed-forward dimension\n",
    "    dropout=0.1,       # Dropout rate\n",
    "    in_dim=7,          # Input feature dimension\n",
    "    context_dim=8,     # Context feature dimension\n",
    ")\n",
    "\n",
    "# Checkpoint path\n",
    "checkpoint_path = \"models/model_1min.pt\"\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_cfm(\n",
    "    train_ds,\n",
    "    val_ds,\n",
    "    model_cfg=model_cfg,\n",
    "    ckpt_path=checkpoint_path,\n",
    "    batch_size=min(2048, max(64, len(train_ds)//8)),  # Adaptive batch size\n",
    "    lr=3e-4,           # Learning rate\n",
    "    epochs=400,        # Maximum epochs\n",
    "    patience=100,      # Early stopping patience\n",
    "    warmup_steps=2000, # Learning rate warmup steps\n",
    "    ema_decay=0.9997,  # EMA decay rate\n",
    ")\n",
    "\n",
    "print(\"Training completed successfully!\")\n",
    "print(f\"Best model saved to: {checkpoint_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
